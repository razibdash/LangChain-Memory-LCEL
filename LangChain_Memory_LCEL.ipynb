{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0de29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_=load_dotenv(find_dotenv())\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1613bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:355: UserWarning: WARNING! max_completion_tokens is not default parameter.\n",
      "                    max_completion_tokens was transferred to model_kwargs.\n",
      "                    Please confirm that max_completion_tokens is what you intended.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:355: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8f4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatprompt template and str output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b85eb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\n**Lionel Messi has a growth hormone deficiency**: Messi was diagnosed with a growth hormone deficiency when he was just 11 years old. This condition affected his growth and development, and he was only 4 feet 7 inches tall at the time. His family couldn't afford the treatment, which cost $1,500 per month, so FC Barcelona offered to pay for his treatment if he joined their youth academy. The rest, as they say, is history!\\n\\nIsn't that an amazing fact?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= prompt | model | output_parser\n",
    "chain.invoke({\"soccer_player\": \"Lionel Messi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391811db",
   "metadata": {},
   "source": [
    "Use of .bind() to add arguments to a Runnable in a LCEL Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a7a36ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nLionel \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= prompt | model.bind(stop=[\"Messi\"]) | output_parser\n",
    "chain.invoke({\"soccer_player\": \"Lionel Messi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e762c1",
   "metadata": {},
   "source": [
    "Combining LCEL Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1c1947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a sentence about Barack Obama:\\n\\nBarack Obama was the 44th President of the United States, serving two terms from 2009 to 2017.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining LCEL Chains\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a sentence about {politician}\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({\"politician\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838c48a",
   "metadata": {},
   "source": [
    "Combined chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9153765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a great question!\\n\\nBarack Obama\\'s presidency was indeed a significant milestone in American history, marking the first time an African American held the office of President. His presidency has had a profound impact on humanity, with many positive outcomes. Here are some examples:\\n\\n1. **Inspiration to People of Color**: Obama\\'s election and presidency served as a beacon of hope and inspiration to people of color, particularly African Americans, who saw themselves reflected in the highest office in the land. It signaled that the doors of opportunity were open to them, and that they too could achieve greatness.\\n2. **Racial Reconciliation**: Obama\\'s presidency helped to bridge the racial divide in America, fostering greater understanding and empathy between people of different racial and ethnic backgrounds. His leadership promoted a sense of unity and inclusivity, which was essential for advancing social justice and equality.\\n3. **Healthcare Reform**: The Affordable Care Act (ACA), also known as Obamacare, was a landmark legislation that expanded healthcare access to millions of previously uninsured Americans. This reform has saved countless lives, improved health outcomes, and reduced healthcare disparities.\\n4. **Economic Recovery**: Obama\\'s presidency coincided with the Great Recession, and his administration\\'s economic policies helped stabilize the economy, stimulate growth, and create jobs. His leadership prevented a potential depression and paved the way for a sustained economic recovery.\\n5. **Climate Change and Energy**: Obama\\'s administration made significant strides in addressing climate change, investing in clean energy, and reducing greenhouse gas emissions. His leadership helped to reposition the United States as a global leader in the fight against climate change.\\n6. **LGBTQ+ Rights**: Obama\\'s presidency was marked by significant advances in LGBTQ+ rights, including the repeal of \"Don\\'t Ask, Don\\'t Tell\" and the legalization of same-sex marriage nationwide.\\n7. **Immigration Reform**: Although comprehensive immigration reform was not achieved during his presidency, Obama\\'s administration implemented the Deferred Action for Childhood Arrivals (DACA) program, which has provided temporary protection and opportunities to hundreds of thousands of young undocumented immigrants.\\n8. **Education**: Obama\\'s administration invested in initiatives to improve education, such as the Race to the Top program, which encouraged innovation and reform in American schools.\\n9. **Foreign Policy**: Obama\\'s presidency saw significant progress in international relations, including the Iran nuclear deal, the normalization of relations with Cuba, and the Paris Agreement on climate change.\\n10. **Leadership and Integrity**: Throughout his presidency, Obama demonstrated strong leadership, integrity, and a commitment to democratic values. He set a high standard for presidential conduct, inspiring confidence and admiration both domestically and internationally.\\n\\nWhile there were certainly challenges and criticisms during Obama\\'s presidency, his overall legacy has had a profoundly positive impact on humanity, promoting hope, unity, and progress in many areas.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historian_prompt = ChatPromptTemplate.from_template(\"Was {politician} positive for Humanity?\")\n",
    "\n",
    "composed_chain = {\"politician\": chain} | historian_prompt | model | StrOutputParser()\n",
    "composed_chain.invoke({\"politician\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49306c",
   "metadata": {},
   "source": [
    "Another example: a chain inside another chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7d440a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama, the 44th President of the United States, is an American by birth and nationality. He was born on August 4, 1961, in Honolulu, Hawaii, USA.\\n\\nSo, to answer your question, Barack Obama is from the United States of America, which is in the continent of উত্তর আমেরিকা (Uttor AmeriKa) or North America.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "chain2=(\n",
    "    {\"country\": chain1,\"language\": itemgetter(\"language\")}\n",
    "    | prompt2 \n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain2.invoke({\"politician\": \"Barack Obama\", \"language\": \"bangla\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f7fbf",
   "metadata": {},
   "source": [
    "LCEL chain at work in a typical RAG app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c83e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13442511",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eaebf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5396\\3866492197.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "#hugging face embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f16f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "retriever= vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ac21a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb6e4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f33cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\chromadb\\types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\chromadb\\types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:665: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. It can be done using various methods, including prompting a large language model (LLM) with simple instructions, using task-specific instructions, or with human inputs. This process helps agents plan ahead and understand what steps are involved in completing a complicated task.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa1e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\chromadb\\types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\chromadb\\types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:665: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A task refers to a larger overall goal or objective, while a subtask is a smaller, more specific step that contributes to achieving the larger task. Task decomposition is the process of breaking down a complex task into smaller, manageable subtasks.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is the difference between a task and a subtask?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
